{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from json import JSONDecodeError\n",
    "from sys import stderr\n",
    "from collections import Counter\n",
    "\n",
    "TEXT_COLUMNS = {\n",
    "    \"all_reviews_texts.txt\": [\"description\"],\n",
    "    \"comments.json\": [\"content\", \"pos_content\", \"neg_content\"],\n",
    "    \"consumers_drugs_reviews.json\": [\"comment\"],\n",
    "    \"doctors_drugs_reviews.json\": [\"comment\", \"comment_plus\", \"comment_minus\"],\n",
    "    \"spr-ru.txt\": [\"text\"]\n",
    "}\n",
    "PROCESS_AS_JSON_DOC = {\n",
    "    \"all_reviews_texts.txt\": False,\n",
    "    \"comments.json\": True,\n",
    "    \"consumers_drugs_reviews.json\": True,\n",
    "    \"doctors_drugs_reviews.json\": True,\n",
    "    \"spr-ru.txt\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To obtain files from PROCESS_AS_JSON_DOC, download the [raw part of the RuDReC corpus](https://yadi.sk/d/kCsAhkoLZUuTrQ) (see the README file). \n",
    "\n",
    "Preprocessing is adopted from:\n",
    "\n",
    "https://github.com/akutuzov/webvectors/blob/master/preprocessing/modular_processing/unify.py\n",
    "\n",
    "We unify letters to decrease the size of dictionary. We also unify and remove all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_replace(search, replacement, text):\n",
    "    \"\"\"\n",
    "    Replaces all symbols of text which are present\n",
    "    in the search string with the replacement string.\n",
    "    \"\"\"\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = list_replace \\\n",
    "        ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "        ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
    "\n",
    "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
    "            '\\u2002', text)\n",
    "\n",
    "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
    "    text = re.sub('\\t\\t', '\\t', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
    "            '.', text)\n",
    "\n",
    "    text = list_replace('\\u2217', '\\u002A', text)\n",
    "\n",
    "    text = list_replace('…', '...', text)\n",
    "\n",
    "    text = list_replace('\\u00C4', 'A', text)\n",
    "    text = list_replace('\\u00E4', 'a', text)\n",
    "    text = list_replace('\\u00CB', 'E', text)\n",
    "    text = list_replace('\\u00EB', 'e', text)\n",
    "    text = list_replace('\\u1E26', 'H', text)\n",
    "    text = list_replace('\\u1E27', 'h', text)\n",
    "    text = list_replace('\\u00CF', 'I', text)\n",
    "    text = list_replace('\\u00EF', 'i', text)\n",
    "    text = list_replace('\\u00D6', 'O', text)\n",
    "    text = list_replace('\\u00F6', 'o', text)\n",
    "    text = list_replace('\\u00DC', 'U', text)\n",
    "    text = list_replace('\\u00FC', 'u', text)\n",
    "    text = list_replace('\\u0178', 'Y', text)\n",
    "    text = list_replace('\\u00FF', 'y', text)\n",
    "    text = list_replace('\\u00DF', 's', text)\n",
    "    text = list_replace('\\u1E9E', 'S', text)\n",
    "    # Removing punctuation\n",
    "    text = list_replace(',.[]{}()=+-−*&^%$#@!~;:§/\\|\\?\"\\n', ' ', text)\n",
    "    # Replacing all numbers with masks\n",
    "    text = list_replace('0123456789', 'x', text)\n",
    "\n",
    "    currencies = list \\\n",
    "            (\n",
    "            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
    "        )\n",
    "\n",
    "    alphabet = list \\\n",
    "            (\n",
    "            '\\t\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "\n",
    "    allowed = set(currencies + alphabet)\n",
    "\n",
    "    cleaned_text = [sym for sym in text if sym in allowed]\n",
    "    cleaned_text = ''.join(cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fulldoc_as_json(input_file, filename, output_file, counter, min_document_length):\n",
    "    \"\"\"\n",
    "    The function reads the whole input_file into memory and\n",
    "    parses it as a one huge JSON list of documents. \n",
    "    \"\"\"\n",
    "    file_text = input_file.read()\n",
    "    docs = json.loads(file_text)\n",
    "    for doc in docs:\n",
    "        for text_field in TEXT_COLUMNS[filename]:\n",
    "            document_text = doc[text_field]\n",
    "            if document_text is not None:\n",
    "                preprocessed_text = clean_text(document_text).lower().strip()\n",
    "                preprocessed_tokens = preprocessed_text.split()\n",
    "                counter.update(preprocessed_tokens)\n",
    "                if len(preprocessed_tokens) >= min_document_length:\n",
    "                    output_file.write(f\"{preprocessed_text}\\n\")\n",
    "\n",
    "\n",
    "def process_jsondoc_linewise(input_file, filename, output_file, counter, min_document_length):\n",
    "    \"\"\"\n",
    "    This function reads input_file line-wise and processes\n",
    "    one JSON document at a time.\n",
    "    \"\"\"\n",
    "    for line in input_file:\n",
    "        try:\n",
    "            line = line.strip('\\n,')\n",
    "            if filename == \"spr-ru.txt\":\n",
    "                doc = ast.literal_eval(line)\n",
    "            else:\n",
    "                doc = json.loads(line)\n",
    "            for text_field in TEXT_COLUMNS[filename]:\n",
    "                document_text = doc[text_field]\n",
    "                if document_text is not None:\n",
    "                    preprocessed_text = clean_text(document_text).lower().strip()\n",
    "                    preprocessed_tokens = preprocessed_text.split()\n",
    "                    counter.update(preprocessed_tokens)\n",
    "                    if len(preprocessed_tokens) >= min_document_length:\n",
    "                        output_file.write(f\"{preprocessed_text}\\n\")\n",
    "        except JSONDecodeError as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines shorter than `MIN_DOCUMENT_LENGTH` are ignored and discarded from the preprocessed corpus. We discard tokens that occur less than `MIN_TOKEN_FREQUENCY` in our corpus. Each line of the file located at `output_path` corresponds to exactly one document. During preprocessing, a temp file with the \"temp\" prefix is created. This file will be removed at the end of preprocessing, do not remove it manually before the preprocessing is finished. The preprocessing is expected to take a few tens of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_directory = r\"RuDReC/\"\n",
    "output_path = \"preprocessed_corpus.txt\"\n",
    "MIN_DOCUMENT_LENGTH = 3 \n",
    "MIN_TOKEN_FREQUENCY = 5\n",
    "counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing all_reviews_texts.txt\n",
      "Preprocessed all_reviews_texts.txt\n",
      "Preprocessing comments.json\n",
      "Preprocessed comments.json\n",
      "Preprocessing consumers_drugs_reviews.json\n",
      "Preprocessed consumers_drugs_reviews.json\n",
      "Preprocessing doctors_drugs_reviews.json\n",
      "Preprocessed doctors_drugs_reviews.json\n",
      "Preprocessing spr-ru.txt\n",
      "Preprocessed spr-ru.txt\n",
      "Filtering rare tokens\n",
      "Finished filtering rare tokens\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(f\"temp_{output_path}\", \"w+\", encoding=\"utf-8\") as output_file:\n",
    "    for filename in os.listdir(corpus_directory):\n",
    "        print(f\"Preprocessing {filename}\")\n",
    "        assert filename in TEXT_COLUMNS.keys()\n",
    "        file_path = os.path.join(corpus_directory, filename)\n",
    "        with codecs.open(file_path, \"r\", encoding=\"utf-8\") as inp:\n",
    "            if PROCESS_AS_JSON_DOC[filename]:\n",
    "                process_fulldoc_as_json(input_file=inp, filename=filename, output_file=output_file, \n",
    "                                        min_document_length=MIN_DOCUMENT_LENGTH, counter=counter)\n",
    "            else:\n",
    "                process_jsondoc_linewise(input_file=inp, filename=filename, output_file=output_file, \n",
    "                                        min_document_length=MIN_DOCUMENT_LENGTH, counter=counter)\n",
    "        print(f\"Preprocessed {filename}\")\n",
    "        \n",
    "print(\"Filtering rare tokens\")\n",
    "with open(f\"temp_{output_path}\", \"r\", encoding=\"utf-8\") as inp_file,\\\n",
    "    open(output_path, \"w+\", encoding=\"utf-8\") as output_file:\n",
    "    for line in inp_file:\n",
    "        new_line = \" \".join([token for token in line.split() if counter[token] >= MIN_TOKEN_FREQUENCY])\n",
    "        output_file.write(f\"{new_line}\\n\")\n",
    "print(\"Finished filtering rare tokens\")\n",
    "            \n",
    "os.remove(f\"temp_{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fasttext in d:\\pf\\anaconda\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: pybind11>=2.2 in d:\\pf\\anaconda\\lib\\site-packages (from fasttext) (2.5.0)\n",
      "Requirement already satisfied: numpy in d:\\pf\\anaconda\\lib\\site-packages (from fasttext) (1.16.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in d:\\pf\\anaconda\\lib\\site-packages (from fasttext) (40.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of all available hyperparameters is available at the official documentation:\n",
    "\n",
    "https://fasttext.cc/docs/en/python-module.html#usage-overview\n",
    "\n",
    "Configuring training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = output_path\n",
    "model_type = 'skipgram' # skipgram or cbow\n",
    "thread = 6 # Number of parallel workers \n",
    "epoch = 10\n",
    "dim=200 # Embedding size\n",
    "model_path = \"fasttext_model.bin\" # The trained model will be saved to this path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Fasttext model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(data_path, model_type, thread=thread,\n",
    "                                    epoch=epoch, dim=dim)\n",
    "\n",
    "model.save_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
